{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 需要导入的包\n",
    "# The python library need be imported\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from transformers import BertTokenizer, BertModel\n",
    "from transformers import AdamW\n",
    "from datasets import load_dataset\n",
    "from datasets import load_from_disk\n",
    "from sklearn import metrics\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 读取数据类\n",
    "# The class of loading datasets\n",
    "class Dataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, split):\n",
    "        self.dataset = load_from_disk('./TikTok')\n",
    "        self.dataset = self.dataset[split]\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "    \n",
    "    def __getitem__(self, i):\n",
    "        text = self.dataset[i]['text']\n",
    "        label = self.dataset[i]['label']\n",
    "        return text, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 加载字典  添加几个不能识别的标点符号\n",
    "# load dictionary and add some punctuation\n",
    "token = BertTokenizer.from_pretrained('bert-base-chinese')\n",
    "token.add_tokens(new_tokens=['…','―','“','”'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 文本最大长度 \n",
    "# Max length of text\n",
    "length = 30\n",
    "\n",
    "# 数据划分\n",
    "# data division to batch\n",
    "def collate_fn(data):\n",
    "    sents = [i[0] for i in data]\n",
    "    labels = [i[1] for i in data]\n",
    "    \n",
    "    data = token.batch_encode_plus(batch_text_or_text_pairs=sents,\n",
    "                                   truncation=True,\n",
    "                                   padding='max_length',\n",
    "                                   max_length=length,\n",
    "                                   return_tensors='pt',\n",
    "                                   return_length=True)\n",
    "\n",
    "    input_ids = data['input_ids']\n",
    "    attention_mask = data['attention_mask']\n",
    "    token_type_ids = data['token_type_ids']\n",
    "    labels = torch.LongTensor(labels)\n",
    "\n",
    "    return input_ids, attention_mask, token_type_ids,labels\n",
    "\n",
    "loader = torch.utils.data.DataLoader(dataset = Dataset('train'),\n",
    "                                     batch_size=16,\n",
    "                                     collate_fn=collate_fn,\n",
    "                                     shuffle=True,\n",
    "                                     drop_last=True)\n",
    "\n",
    "for i, (input_ids, attention_mask, token_type_ids,labels) in enumerate(loader):\n",
    "    break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-chinese were not used when initializing BertModel: ['cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "# 从huggingface加载BERT模型 \n",
    "# From huggingface load BERT model\n",
    "pretrained = BertModel.from_pretrained('bert-base-chinese')\n",
    "pretrained.resize_token_embeddings(len(token))\n",
    "for param in pretrained.parameters():\n",
    "    param.requires_grad_(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([16, 2])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 使用lstm为分类器 \n",
    "# Creat a LSTM for classifcation\n",
    "class LSTM(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(LSTM,self).__init__()\n",
    "        self.rnn = nn.LSTM(768, 384, num_layers=2, batch_first= True,bidirectional=True, dropout=0.5)\n",
    "        self.fc = torch.nn.Linear(384*2, 2)\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, token_type_ids):\n",
    "        with torch.no_grad():\n",
    "            out = pretrained(input_ids=input_ids,\n",
    "                       attention_mask=attention_mask,\n",
    "                       token_type_ids=token_type_ids)\n",
    "        output, (hidden, cell) = self.rnn(out.last_hidden_state)\n",
    "        hidden = torch.cat([hidden[-2],hidden[-1]], dim=1)\n",
    "        hidden = self.dropout(hidden)\n",
    "        out = self.fc(hidden)\n",
    "    \n",
    "        return out\n",
    "\n",
    "\n",
    "model1 = LSTM()\n",
    "\n",
    "model1(input_ids=input_ids,\n",
    "      attention_mask=attention_mask,\n",
    "      token_type_ids=token_type_ids).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\zhenyu\\AppData\\Local\\anaconda3\\envs\\NLP\\lib\\site-packages\\transformers\\optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0.6786486506462097 0.6875\n",
      "5 0.5608528256416321 0.6875\n",
      "10 0.4853718876838684 0.8125\n",
      "15 0.382571280002594 0.8125\n",
      "20 0.7550916075706482 0.6875\n",
      "25 0.389615535736084 0.8125\n",
      "30 0.6661620140075684 0.6875\n",
      "35 0.2125944346189499 0.9375\n",
      "40 0.9701465368270874 0.6875\n",
      "45 0.328982949256897 0.875\n",
      "50 0.3119659125804901 0.875\n",
      "55 0.12042928487062454 0.9375\n",
      "60 0.41816550493240356 0.6875\n",
      "65 0.16959929466247559 0.9375\n",
      "70 0.45900094509124756 0.875\n",
      "75 0.11932005733251572 0.9375\n",
      "80 0.37557628750801086 0.8125\n",
      "85 0.29105183482170105 0.875\n",
      "90 0.3299608528614044 0.875\n",
      "95 0.16580957174301147 0.9375\n",
      "100 0.1903885155916214 0.875\n",
      "105 0.30719897150993347 0.875\n",
      "110 0.4996291697025299 0.75\n",
      "115 0.20678968727588654 0.875\n",
      "120 0.21921245753765106 0.875\n",
      "125 0.16400021314620972 0.9375\n",
      "130 0.15519458055496216 0.9375\n",
      "135 0.20067480206489563 0.9375\n",
      "140 0.38900670409202576 0.8125\n",
      "145 0.24437777698040009 0.875\n",
      "150 0.056191351264715195 1.0\n",
      "155 0.14547888934612274 0.9375\n",
      "160 0.1203714907169342 0.9375\n",
      "165 0.17818517982959747 0.9375\n",
      "170 0.08137977123260498 1.0\n",
      "175 0.10669828206300735 1.0\n",
      "180 0.21815167367458344 0.875\n",
      "185 0.36576586961746216 0.8125\n",
      "190 0.22054795920848846 0.9375\n",
      "195 0.13916344940662384 0.9375\n",
      "200 0.5325824022293091 0.875\n",
      "205 0.29962587356567383 0.875\n",
      "210 0.2548629641532898 0.9375\n",
      "215 0.3350134491920471 0.75\n",
      "220 0.07694535702466965 0.9375\n",
      "225 0.07960473001003265 0.9375\n"
     ]
    }
   ],
   "source": [
    "# 训练 \n",
    "# Training\n",
    "optimizer = AdamW(model1.parameters(), lr=5e-4)\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "model1.train()\n",
    "for i, (input_ids, attention_mask, token_type_ids, labels) in enumerate(loader):\n",
    "    out = model1(input_ids=input_ids,\n",
    "                attention_mask=attention_mask,\n",
    "                token_type_ids=token_type_ids)\n",
    "    loss = criterion(out, labels)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    if i % 5 == 0:\n",
    "        out = out.argmax(dim=1)\n",
    "        accuracy = (out == labels).sum().item() / len(labels)\n",
    "        print(i, loss.item(), accuracy)\n",
    "\n",
    "    if i == 450:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 92.00%\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0     0.8373    0.9392    0.8854       148\n",
      "         1.0     0.9683    0.9106    0.9386       302\n",
      "\n",
      "    accuracy                         0.9200       450\n",
      "   macro avg     0.9028    0.9249    0.9120       450\n",
      "weighted avg     0.9252    0.9200    0.9211       450\n",
      "\n",
      "[[139   9]\n",
      " [ 27 275]]\n"
     ]
    }
   ],
   "source": [
    "# 测试 \n",
    "# Testing \n",
    "def test():\n",
    "    model1.eval()\n",
    "    correct = 0\n",
    "\n",
    "\n",
    "    loader_test = torch.utils.data.DataLoader(dataset=Dataset('test'),\n",
    "                                              batch_size=1,\n",
    "                                              collate_fn=collate_fn,\n",
    "                                              shuffle=True,\n",
    "                                              drop_last=True)\n",
    "    labels_all=[]\n",
    "    pred_all=[]\n",
    "\n",
    "    for i, (input_ids, attention_mask, token_type_ids,labels) in enumerate(loader_test):\n",
    "        \n",
    "        if i == 450:\n",
    "            break\n",
    "        with torch.no_grad():\n",
    "            out = model1(input_ids=input_ids,\n",
    "                        attention_mask=attention_mask,\n",
    "                        token_type_ids=token_type_ids)\n",
    "\n",
    "        out = out.argmax(dim=1)\n",
    "        correct += (out == labels).sum().item()\n",
    "        labels_all=np.append(labels_all,labels)\n",
    "        pred_all=np.append(pred_all,out)    \n",
    "    accuracy = metrics.accuracy_score(labels_all, pred_all)\n",
    "    print(\"Accuracy: %.2f%%\" % (accuracy * 100.0))\n",
    "    print(metrics.classification_report(labels_all, pred_all,digits=4) ) \n",
    "    print(metrics.confusion_matrix(labels_all, pred_all))\n",
    "test()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
